{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PythiaBNS: Robust BNS Post-Merger Parameter Estimation","text":"<p>PythiaBNS is a modular Python library designed for Bayesian Parameter Estimation (PE) of Binary Neutron Star (BNS) post-merger waveforms. It is architected to support next-generation (3G) gravitational wave observatories like Einstein Telescope (ET) and Cosmic Explorer (CE).</p> <p>PythiaBNS implements the method of Empirical Priors of Vretinaris et al. 2026, where a set of informed priors are used to constrain Parameter Estimation of post-merger waveforms leading to robust and fast inference.</p>"},{"location":"#key-use-cases","title":"Key Use Cases","text":"<ul> <li>Analytic Modeling: Fit complex time-domain or frequency-domain waveform models to Numerical Relativity (NR) data.</li> <li>Inspiral-Informed Priors: Constrain post-merger parameters using empirical relations derived from inspiral measurements (mass, tidal deformability).</li> <li>High-Efficiency Sampling: Leverage <code>pocomc</code> (Preconditioned Monte Carlo) for efficient sampling of difficult posteriors.</li> <li>Benchmarking: Validate models against a curated catalog of NR waveforms.</li> </ul>"},{"location":"#quick-install","title":"Quick Install","text":"<p>This project is managed with uv.</p> <pre><code># Clone the repository\ngit clone https://github.com/svretina/pythiabns.git\ncd pythiabns\n\n# Install dependencies\nuv sync\n</code></pre> <p>See the Usage guide for detailed instructions.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use PythiaBNS in your research, please cite:</p> <pre><code>@article{g1qs-j74x,\n  title = {Robust and fast parameter estimation for gravitational waves from binary neutron star merger remnants},\n  author = {Vretinaris, Stamatis and Vretinaris, Georgios and Mermigkas, Christos and Karamanis, Minas and Stergioulas, Nikolaos},\n  journal = {Phys. Rev. D},\n  volume = {113},\n  issue = {2},\n  pages = {024012},\n  year = {2026},\n  doi = {10.1103/PhysRevD.113.024012},\n  url = {https://link.aps.org/doi/10.1103/PhysRevD.113.024012}\n}\n</code></pre>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#core","title":"Core","text":""},{"location":"api/#pythiabns.core.config.InjectionConfig","title":"<code>InjectionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for waveform injection source.</p> Source code in <code>src/pythiabns/core/config.py</code> <pre><code>class InjectionConfig(BaseModel):\n    \"\"\"Configuration for waveform injection source.\"\"\"\n    # \"nr\": load from STRAIN_PATH via ID\n    # \"file\": load from absolute/relative path\n    # \"analytic\": simulate using a model from registry\n    mode: str = \"nr\"\n    target: Optional[str] = None # NR ID, File Path, or Model Name\n    parameters: Dict[str, float] = Field(default_factory=dict)\n</code></pre>"},{"location":"api/#pythiabns.core.config.JobMatrix","title":"<code>JobMatrix</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for expanding into multiple simulations.</p> Source code in <code>src/pythiabns/core/config.py</code> <pre><code>class JobMatrix(BaseModel):\n    \"\"\"Configuration for expanding into multiple simulations.\"\"\"\n    # Support both legacy 'waveform' and new modular 'injection'\n    waveform: Optional[List[str]] = None\n    injection: Optional[List[InjectionConfig]] = None\n\n    snr: List[float] = Field(default_factory=lambda: [50.0])\n    model: List[str]\n\n    sampler: SamplerConfig\n    priors: PriorConfig\n\n    # Extra model parameters like nfreqs\n    model_params: Dict[str, Any] = Field(default_factory=dict)\n    # Legacy: injection_parameters at top level\n    injection_parameters: Dict[str, float] = Field(default_factory=dict)\n</code></pre>"},{"location":"api/#pythiabns.core.config.SimulationConfig","title":"<code>SimulationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a single simulation run.</p> Source code in <code>src/pythiabns/core/config.py</code> <pre><code>class SimulationConfig(BaseModel):\n    \"\"\"Configuration for a single simulation run.\"\"\"\n    # Single instance of injection source\n    injection: InjectionConfig\n\n    snr: float\n    model: str\n    sampler: SamplerConfig\n    priors: PriorConfig\n\n    model_params: Dict[str, Any] = Field(default_factory=dict)\n    # Legacy support\n    waveform: Optional[str] = None \n    injection_parameters: Dict[str, float] = Field(default_factory=dict)\n</code></pre>"},{"location":"api/#pythiabns.core.registry.Registry","title":"<code>Registry</code>","text":"<p>Generic registry for models, relations, etc. supporting metadata and variants.</p> Source code in <code>src/pythiabns/core/registry.py</code> <pre><code>class Registry:\n    \"\"\"Generic registry for models, relations, etc. supporting metadata and variants.\"\"\"\n\n    def __init__(self):\n        # Store as list of candidates: {name: [{'obj': obj, 'meta': meta}, ...]}\n        self._registry: Dict[str, List[Dict[str, Any]]] = {}\n\n    def register(self, name: str, **metadata):\n        \"\"\"Decorator to register a class or function with optional metadata.\"\"\"\n        def decorator(obj):\n            if name not in self._registry:\n                self._registry[name] = []\n\n            # Check for duplicates? or just append?\n            # Append allows variants.\n            entry = {'obj': obj, 'meta': metadata}\n            self._registry[name].append(entry)\n            logger.debug(f\"Registered {name} with metadata {metadata}\")\n            return obj\n        return decorator\n\n    def get(self, name: str, **filters) -&gt; Optional[Any]:\n        \"\"\"\n        Get object by name, optionally filtering by metadata.\n        Example: get(\"lorentzian\", nfreqs=3)\n        \"\"\"\n        candidates = self._registry.get(name, [])\n        if not candidates:\n            return None\n\n        matches = []\n        for cand in candidates:\n            meta = cand['meta']\n            # Check if all filters match metadata\n            # If filter key not in meta, mismatch? \n            # Or if meta has key and filter matches.\n            is_match = True\n            for k, v in filters.items():\n                if k not in meta or meta[k] != v:\n                    is_match = False\n                    break\n            if is_match:\n                matches.append(cand)\n\n        if len(matches) == 0:\n            logger.warning(f\"No match found for {name} with filters {filters} among {len(candidates)} candidates.\")\n            return None\n        elif len(matches) &gt; 1:\n            # Ambiguous? Return last registered or first?\n            # Original code seemed to rely on overwriting or specific key.\n            # We return the last one (most recently registered)\n            logger.debug(f\"Multiple matches for {name}, returning last one.\")\n            return matches[-1]['obj']\n        else:\n            return matches[0]['obj']\n\n    def get_metadata(self, name: str, **filters) -&gt; Dict[str, Any]:\n        candidates = self._registry.get(name, [])\n        # Same logic as get, but return meta\n        if not candidates: return {}\n\n        matches = []\n        for cand in candidates:\n            meta = cand['meta']\n            is_match = True\n            for k, v in filters.items():\n                if k not in meta or meta[k] != v:\n                    is_match = False\n                    break\n            if is_match:\n                matches.append(cand)\n\n        if matches:\n            return matches[-1]['meta']\n        return {}\n\n    def list_available(self) -&gt; list:\n        return list(self._registry.keys())\n</code></pre>"},{"location":"api/#pythiabns.core.registry.Registry.get","title":"<code>get(name, **filters)</code>","text":"<p>Get object by name, optionally filtering by metadata. Example: get(\"lorentzian\", nfreqs=3)</p> Source code in <code>src/pythiabns/core/registry.py</code> <pre><code>def get(self, name: str, **filters) -&gt; Optional[Any]:\n    \"\"\"\n    Get object by name, optionally filtering by metadata.\n    Example: get(\"lorentzian\", nfreqs=3)\n    \"\"\"\n    candidates = self._registry.get(name, [])\n    if not candidates:\n        return None\n\n    matches = []\n    for cand in candidates:\n        meta = cand['meta']\n        # Check if all filters match metadata\n        # If filter key not in meta, mismatch? \n        # Or if meta has key and filter matches.\n        is_match = True\n        for k, v in filters.items():\n            if k not in meta or meta[k] != v:\n                is_match = False\n                break\n        if is_match:\n            matches.append(cand)\n\n    if len(matches) == 0:\n        logger.warning(f\"No match found for {name} with filters {filters} among {len(candidates)} candidates.\")\n        return None\n    elif len(matches) &gt; 1:\n        # Ambiguous? Return last registered or first?\n        # Original code seemed to rely on overwriting or specific key.\n        # We return the last one (most recently registered)\n        logger.debug(f\"Multiple matches for {name}, returning last one.\")\n        return matches[-1]['obj']\n    else:\n        return matches[0]['obj']\n</code></pre>"},{"location":"api/#pythiabns.core.registry.Registry.register","title":"<code>register(name, **metadata)</code>","text":"<p>Decorator to register a class or function with optional metadata.</p> Source code in <code>src/pythiabns/core/registry.py</code> <pre><code>def register(self, name: str, **metadata):\n    \"\"\"Decorator to register a class or function with optional metadata.\"\"\"\n    def decorator(obj):\n        if name not in self._registry:\n            self._registry[name] = []\n\n        # Check for duplicates? or just append?\n        # Append allows variants.\n        entry = {'obj': obj, 'meta': metadata}\n        self._registry[name].append(entry)\n        logger.debug(f\"Registered {name} with metadata {metadata}\")\n        return obj\n    return decorator\n</code></pre>"},{"location":"api/#pythiabns.core.plotting.generate_plots","title":"<code>generate_plots(result, config, outdir)</code>","text":"<p>Generate plots based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <p>bilby.core.result.Result object</p> required <code>config</code> <p>PlottingConfig object</p> required <code>outdir</code> <code>Path</code> <p>Path to output directory</p> required Source code in <code>src/pythiabns/core/plotting.py</code> <pre><code>def generate_plots(result, config, outdir: Path):\n    \"\"\"\n    Generate plots based on configuration.\n\n    Args:\n        result: bilby.core.result.Result object\n        config: PlottingConfig object\n        outdir: Path to output directory\n    \"\"\"\n    if not config.enabled:\n        return\n\n    outdir = Path(outdir)\n    outdir.mkdir(exist_ok=True, parents=True) # Should already exist but good check\n\n    for plot_type in config.plots:\n        try:\n            if plot_type == \"corner\":\n                plot_corner(result, outdir, **config.settings.get(\"corner\", {}))\n            elif plot_type == \"trace\":\n                plot_trace(result, outdir, **config.settings.get(\"trace\", {}))\n            elif plot_type == \"waveform\":\n                # This requires the waveform generator which isn't passed here easily\n                # typically stored in result object metadata if using standard bilby\n                # or we skip for now/handle in spine\n                logger.warning(\"Waveform plotting from this module requires access to the generator. Handled in spine.py?\")\n            else:\n                logger.warning(f\"Unknown plot type: {plot_type}\")\n        except Exception as e:\n            logger.error(f\"Failed to generate {plot_type} plot: {e}\")\n</code></pre>"},{"location":"api/#pythiabns.core.plotting.plot_waveform_posterior","title":"<code>plot_waveform_posterior(result, waveform_generator, descriptors, outdir, n_samples=100)</code>","text":"<p>Plot the waveform posterior against injection/data.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <p>Bilby Result</p> required <code>waveform_generator</code> <p>Initiailized WaveformGenerator</p> required <code>descriptors</code> <p>Dictionary of {'label': [list of ifos]} or similar to know what to plot?          Actually, usually we plot strain vs time/freq.</p> required <code>outdir</code> <p>Output path</p> required Source code in <code>src/pythiabns/core/plotting.py</code> <pre><code>def plot_waveform_posterior(result, waveform_generator, descriptors, outdir, n_samples=100):\n    \"\"\"\n    Plot the waveform posterior against injection/data.\n\n    Args:\n        result: Bilby Result\n        waveform_generator: Initiailized WaveformGenerator\n        descriptors: Dictionary of {'label': [list of ifos]} or similar to know what to plot?\n                     Actually, usually we plot strain vs time/freq.\n        outdir: Output path\n    \"\"\"\n    logger.info(\"Generating waveform posterior plot...\")\n\n    # Select random samples\n    if len(result.posterior) &lt; n_samples:\n        samples = result.posterior\n    else:\n        samples = result.posterior.sample(n=n_samples)\n\n    # We need to compute waveforms for these samples\n    # accessing waveform_generator.time_domain_source_model(parameter)\n\n    # This is quite specific to the domain (time/freq) and IFOs\n    # For now, let's just implement a simple Time Domain viewer if available\n\n    pass\n</code></pre>"},{"location":"api/#models","title":"Models","text":""},{"location":"api/#pythiabns.models.relations.EmpiricalRelation","title":"<code>EmpiricalRelation</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for universal relations.</p> Source code in <code>src/pythiabns/models/relations.py</code> <pre><code>class EmpiricalRelation(ABC):\n    \"\"\"Abstract base class for universal relations.\"\"\"\n\n    @abstractmethod\n    def predict(self, m1: float, m2: float, eos_name: str) -&gt; Dict[str, float]:\n        \"\"\"\n        Predict f_peak and potentially other properties.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/#pythiabns.models.relations.EmpiricalRelation.predict","title":"<code>predict(m1, m2, eos_name)</code>  <code>abstractmethod</code>","text":"<p>Predict f_peak and potentially other properties.</p> Source code in <code>src/pythiabns/models/relations.py</code> <pre><code>@abstractmethod\ndef predict(self, m1: float, m2: float, eos_name: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Predict f_peak and potentially other properties.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#pythiabns.models.interface.WaveformModel","title":"<code>WaveformModel</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol that all waveform models must adhere to.</p> Source code in <code>src/pythiabns/models/interface.py</code> <pre><code>@runtime_checkable\nclass WaveformModel(Protocol):\n    \"\"\"Protocol that all waveform models must adhere to.\"\"\"\n\n    def __call__(self, frequency_array: np.ndarray, **params: float) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Generate waveform polarizations.\n\n        Args:\n            frequency_array: Array of frequencies in Hz.\n            **params: Model parameters (masses, spins, etc).\n\n        Returns:\n            Dict containing 'plus' and 'cross' keys with complex strain arrays.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/#pythiabns.models.interface.WaveformModel.__call__","title":"<code>__call__(frequency_array, **params)</code>","text":"<p>Generate waveform polarizations.</p> <p>Parameters:</p> Name Type Description Default <code>frequency_array</code> <code>ndarray</code> <p>Array of frequencies in Hz.</p> required <code>**params</code> <code>float</code> <p>Model parameters (masses, spins, etc).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict containing 'plus' and 'cross' keys with complex strain arrays.</p> Source code in <code>src/pythiabns/models/interface.py</code> <pre><code>def __call__(self, frequency_array: np.ndarray, **params: float) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Generate waveform polarizations.\n\n    Args:\n        frequency_array: Array of frequencies in Hz.\n        **params: Model parameters (masses, spins, etc).\n\n    Returns:\n        Dict containing 'plus' and 'cross' keys with complex strain arrays.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#inference","title":"Inference","text":""},{"location":"api/#pythiabns.inference.priors.PriorFactory","title":"<code>PriorFactory</code>","text":"<p>Factory to generate bilby PriorDicts based on configuration.</p> Source code in <code>src/pythiabns/inference/priors.py</code> <pre><code>class PriorFactory:\n    \"\"\"Factory to generate bilby PriorDicts based on configuration.\"\"\"\n\n    @staticmethod\n    def create_priors(config: PriorConfig, \n                      model_name: str, \n                      metadata: Optional[Dict[str, Any]] = None,\n                      model_params: Dict[str, Any] = None) -&gt; bilby.core.prior.PriorDict:\n\n        # Fetch conversion function from registry\n        reg_meta = registry.ModelRegistry.get_metadata(model_name, **(model_params or {}))\n        conversion_func = reg_meta.get(\"conversion_func\")\n\n        # 1. Load base priors\n        priors = bilby.core.prior.PriorDict(conversion_function=conversion_func)\n\n        # Load from file if specified\n        base_filename = f\"{model_name}.priors\"\n        # Check explicit source or default locations\n        if config.source:\n             if (constants.PRIORS_PATH / config.source).exists():\n                 priors.from_file(str(constants.PRIORS_PATH / config.source))\n             elif (constants.PROJECT_ROOT / config.source).exists():\n                 priors.from_file(str(constants.PROJECT_ROOT / config.source))\n\n        if not priors and (constants.PRIORS_PATH / base_filename).exists():\n            priors.from_file(str(constants.PRIORS_PATH / base_filename))\n\n        # 2. Apply Empirical Relations if requested\n        if config.mode == \"empirical\" and metadata:\n            PriorFactory._apply_empirical_relations(priors, config, metadata)\n\n        return priors\n\n    @staticmethod\n    def _apply_empirical_relations(priors: bilby.core.prior.PriorDict, \n                                   config: PriorConfig, \n                                   metadata: Dict[str, Any]):\n\n        method = config.source # e.g. \"VSB_R\"\n        relation_cls = registry.RelationRegistry.get(method)\n        if not relation_cls:\n            logger.warning(f\"Relation {method} not found in registry.\")\n            return\n\n        relation = relation_cls()\n        eos = metadata.get(\"id_eos\", \"SLY\")\n        m1 = metadata.get(\"id_mass_starA\")\n        m2 = metadata.get(\"id_mass_starB\")\n\n        if m1 is None or m2 is None:\n            logger.warning(\"Masses not found in metadata, skipping empirical priors.\")\n            return\n\n        preds = relation.predict(m1, m2, eos)\n        fpeak = preds.get(\"f_peak\")\n\n        # Update f_peak prior\n        if \"f_peak\" in priors and fpeak:\n            # Assume 10% width or similar logic from original\n            # Original used specific logic based on Distribution type (Gaussian/Uniform)\n            # Here we simplify or need to inspect prior type.\n            prior = priors[\"f_peak\"]\n            if isinstance(prior, bilby.core.prior.Uniform):\n                width = 500 # Arbitrary default or derived?\n                priors[\"f_peak\"] = bilby.core.prior.Uniform(fpeak - width, fpeak + width, name=\"f_peak\")\n            elif isinstance(prior, bilby.core.prior.Gaussian):\n                priors[\"f_peak\"] = bilby.core.prior.Gaussian(mu=fpeak, sigma=200, name=\"f_peak\") # sigma??\n</code></pre>"},{"location":"api/#pythiabns.inference.likelihood.PostMergerLikelihood","title":"<code>PostMergerLikelihood</code>","text":"<p>               Bases: <code>GravitationalWaveTransient</code></p> <p>Likelihood class for Post-Merger PE. Inherits from GravitationalWaveTransient to use standard detector response logic.</p> Source code in <code>src/pythiabns/inference/likelihood.py</code> <pre><code>class PostMergerLikelihood(bilby.gw.likelihood.GravitationalWaveTransient):\n    \"\"\"\n    Likelihood class for Post-Merger PE.\n    Inherits from GravitationalWaveTransient to use standard detector response logic.\n    \"\"\"\n    def __init__(self, interferometers, waveform_generator):\n        super().__init__(interferometers, waveform_generator)\n\n    # Note: The standard GravitationalWaveTransient is usually sufficient \n    # IF the waveform_generator produces the correct mode (FD/TD).\n    # However, we might want to override logic if we need custom handling meant for 3G detectors \n    # or specific noise models not in Bilby (though Bilby is quite complete).\n\n    # For now, this is a thin wrapper but allows us to extend it later \n    # (e.g. for marginalization over calibration if needed).\n    pass\n</code></pre>"},{"location":"api/#samplers","title":"Samplers","text":""},{"location":"api/#pythiabns.inference.samplers.pocomc.BilbyPocomcPrior","title":"<code>BilbyPocomcPrior</code>","text":"<p>Wrapper to make bilby priors compatible with PocoMC Prior interface.</p> Source code in <code>src/pythiabns/inference/samplers/pocomc.py</code> <pre><code>class BilbyPocomcPrior:\n    \"\"\"Wrapper to make bilby priors compatible with PocoMC Prior interface.\"\"\"\n    def __init__(self, bilby_priors):\n        self.priors = bilby_priors\n\n        # Identify stochastic keys\n        self.keys = []\n        self.fixed_params = {}\n\n        for k in self.priors.keys():\n            p = self.priors[k]\n            if isinstance(p, (bilby.core.prior.Constraint, bilby.core.prior.DeltaFunction)):\n                if isinstance(p, bilby.core.prior.DeltaFunction):\n                    self.fixed_params[k] = p.peak\n                continue\n            elif isinstance(p, (float, int)):\n                self.fixed_params[k] = p\n                continue\n            else:\n                self.keys.append(k)\n\n        self.dim = len(self.keys)\n        # Sort keys to ensure consistency\n        self.keys.sort()\n\n        # Precompute bounds\n        self._bounds = []\n        for k in self.keys:\n             p = self.priors[k]\n             if hasattr(p, 'minimum') and hasattr(p, 'maximum'):\n                 self._bounds.append([p.minimum, p.maximum])\n             else:\n                 self._bounds.append([-np.inf, np.inf])\n        self.bounds = np.array(self._bounds)\n\n    def logpdf(self, x):\n        x = np.atleast_2d(x)\n        n = x.shape[0]\n        res = np.zeros(n)\n\n        for i in range(n):\n            params = dict(zip(self.keys, x[i]))\n            # Merge fixed params so constraints/dependencies work\n            params.update(self.fixed_params) \n            res[i] = self.priors.ln_prob(params)\n        return res\n\n    def rvs(self, size=1):\n        samples = [self.priors.sample() for _ in range(size)]\n        res = np.array([[s[k] for k in self.keys] for s in samples])\n        return res\n\n    def __call__(self, x):\n        return self.logpdf(x)\n</code></pre>"},{"location":"api/#pythiabns.inference.samplers.pocomc.PocoMCWrapper","title":"<code>PocoMCWrapper</code>","text":"<p>Wrapper for PocoMC sampler.</p> Source code in <code>src/pythiabns/inference/samplers/pocomc.py</code> <pre><code>class PocoMCWrapper:\n    \"\"\"Wrapper for PocoMC sampler.\"\"\"\n\n    def __init__(self, likelihood: bilby.Likelihood, \n                 priors: bilby.core.prior.PriorDict,\n                 outdir: Path,\n                 label: str,\n                 settings: Dict[str, Any] = None):\n\n        self.likelihood = likelihood\n        self.priors = priors\n        self.outdir = Path(outdir)\n        self.label = label\n        self.settings = settings or {}\n\n        self.outdir.mkdir(parents=True, exist_ok=True)\n        self.wrapped_prior = BilbyPocomcPrior(self.priors)\n\n        # Populate fixed parameters into likelihood initially\n        self.likelihood.parameters.update(self.wrapped_prior.fixed_params)\n\n        # Periodicity logic\n        self.periodicity = []\n        for i, key in enumerate(self.wrapped_prior.keys):\n            p = self.priors[key]\n            if hasattr(p, 'boundary') and p.boundary == 'periodic':\n                self.periodicity.append(i)\n\n    def log_likelihood(self, x):\n        x = np.atleast_1d(x)\n        if x.ndim &gt; 1:\n             res = []\n             for xi in x:\n                 res.append(self._log_likelihood_single(xi))\n             return np.array(res)\n        return self._log_likelihood_single(x)\n\n    def _log_likelihood_single(self, x):\n        params = dict(zip(self.wrapped_prior.keys, x))\n        # Fixed params already in likelihood.parameters, but safety:\n        # If likelihood code modifies them? Usually not.\n        # But if we update with `params`, it overwrites stochastic ones.\n        self.likelihood.parameters.update(params)\n        return self.likelihood.log_likelihood() \n\n    def run(self):\n        import pocomc as pc\n\n        nwalkers = self.settings.get(\"npoints\", 1000)\n\n        # Using 'multiprocess' with dill support\n        with multiprocess.Pool(self.settings.get(\"n_cpus\", 1)) as pool:\n            sampler = pc.Sampler(\n                prior=self.wrapped_prior,\n                likelihood=self.log_likelihood,\n                n_dim=self.wrapped_prior.dim,\n                n_effective=nwalkers,\n                n_active=nwalkers,\n                periodic=self.periodicity,\n                pool=pool,\n                vectorize=False\n            )\n\n            sampler.run(progress=True)\n\n            self._save_results(sampler)\n\n    def _save_results(self, sampler):\n        results = sampler.results\n        samples = results.get('samples', results.get('posterior_samples'))\n        if samples is None:\n             # Fallback logic based on version behavior\n             if 'posterior_samples' in results and results['posterior_samples'] is not None:\n                 samples = results['posterior_samples']\n             else:\n                 # sampler might have 'samples' attribute?\n                 pass\n\n        if samples is not None:\n            df = pd.DataFrame(samples, columns=self.wrapped_prior.keys)\n            df['log_prior'] = results.get('posterior_logp', results.get('log_prior'))\n            df['log_likelihood'] = results.get('posterior_logl', results.get('log_likelihood'))\n\n            df.to_json(self.outdir / f\"{self.label}_result.json\")\n\n        with open(self.outdir / f\"{self.label}_pocomc.pickle\", \"wb\") as f:\n            dill.dump(results, f)\n</code></pre>"},{"location":"api/#pythiabns.inference.samplers.zeus.ZeusWrapper","title":"<code>ZeusWrapper</code>","text":"<p>Wrapper for zeus sampler.</p> Source code in <code>src/pythiabns/inference/samplers/zeus.py</code> <pre><code>class ZeusWrapper:\n    \"\"\"Wrapper for zeus sampler.\"\"\"\n\n    def __init__(self, likelihood: bilby.Likelihood, \n                 priors: bilby.core.prior.PriorDict,\n                 outdir: Path,\n                 label: str,\n                 settings: Dict[str, Any] = None):\n\n        self.likelihood = likelihood\n        self.priors = priors\n        self.outdir = Path(outdir)\n        self.label = label\n        self.settings = settings or {}\n\n        self.outdir.mkdir(parents=True, exist_ok=True)\n        self.wrapped_prior = BilbyPocomcPrior(self.priors) # Zeus also needs prior samples/logpdf\n\n        self.likelihood.parameters.update(self.wrapped_prior.fixed_params)\n\n    def log_likelihood(self, x):\n        params = dict(zip(self.wrapped_prior.keys, x))\n        self.likelihood.parameters.update(params)\n        return self.likelihood.log_likelihood()\n\n    def run(self):\n        import zeus\n        import multiprocess\n\n        nwalkers = self.settings.get(\"nwalkers\", 2 * self.wrapped_prior.dim + 2)\n        nsteps = self.settings.get(\"nsteps\", 1000)\n        n_cpus = self.settings.get(\"n_cpus\", 1)\n\n        # Initial positions\n        start_pos = self.wrapped_prior.rvs(nwalkers)\n\n        with multiprocess.Pool(n_cpus) as pool:\n            sampler = zeus.Sampler(\n                logprob_fn=self.log_likelihood,\n                n_dim=self.wrapped_prior.dim,\n                n_walkers=nwalkers,\n                pool=pool\n            )\n\n            sampler.run_mcmc(start_pos, nsteps)\n\n            self._save_results(sampler)\n\n    def _save_results(self, sampler):\n        samples = sampler.get_chain(flat=True)\n        log_prob = sampler.get_log_prob(flat=True)\n\n        df = pd.DataFrame(samples, columns=self.wrapped_prior.keys)\n        df['log_likelihood'] = log_prob # Note: zeus doesn't separate prior/likelihood by default if using logprob_fn\n\n        df.to_json(self.outdir / f\"{self.label}_result.json\")\n\n        with open(self.outdir / f\"{self.label}_zeus.pickle\", \"wb\") as f:\n            dill.dump(sampler, f)\n</code></pre>"},{"location":"api/#pythiabns.inference.samplers.numpyro_sampler.NumPyroWrapper","title":"<code>NumPyroWrapper</code>","text":"<p>Wrapper for NumPyro sampler.</p> Source code in <code>src/pythiabns/inference/samplers/numpyro_sampler.py</code> <pre><code>class NumPyroWrapper:\n    \"\"\"Wrapper for NumPyro sampler.\"\"\"\n\n    def __init__(self, likelihood: bilby.Likelihood, \n                 priors: bilby.core.prior.PriorDict,\n                 outdir: Path,\n                 label: str,\n                 settings: Dict[str, Any] = None):\n\n        self.likelihood = likelihood\n        self.priors = priors\n        self.outdir = Path(outdir)\n        self.label = label\n        self.settings = settings or {}\n\n        self.outdir.mkdir(parents=True, exist_ok=True)\n        self.wrapped_prior = BilbyPocomcPrior(self.priors)\n\n        self.likelihood.parameters.update(self.wrapped_prior.fixed_params)\n\n    def log_likelihood(self, x):\n        params = dict(zip(self.wrapped_prior.keys, x))\n        self.likelihood.parameters.update(params)\n        return self.likelihood.log_likelihood()\n\n    def run(self):\n        import jax\n        import jax.numpy as jnp\n        import numpyro\n        import numpyro.distributions as dist\n        from numpyro.infer import MCMC, NUTS, Predictive\n\n        n_samples = self.settings.get(\"n_samples\", 1000)\n        n_warmup = self.settings.get(\"n_warmup\", n_samples // 2)\n        n_chains = self.settings.get(\"n_chains\", 1)\n\n        # We use a potential_fn because we have a custom log-likelihood/prior\n        def potential_fn(params):\n            # Convert jax dict to list in correct order\n            x = [params[k] for k in self.wrapped_prior.keys]\n\n            # Use pure_callback to call non-jax likelihood\n            def wrapped_logp(x_np):\n                lp = self.wrapped_prior.logpdf(np.atleast_2d(x_np))[0]\n                ll = self.log_likelihood(x_np)\n                return -(lp + ll)\n\n            val = jax.pure_callback(wrapped_logp, jnp.float64(0.0), jnp.array(x))\n            return val\n\n        # For NUTS, we need gradients. pure_callback doesn't support them by default.\n        # If we use NUTS, it might fail unless we use a sampler that doesn't need gradients \n        # or we provide numerical gradients.\n\n        logger.info(f\"NumPyro sampling started with {n_samples} samples and {n_chains} chains.\")\n\n        # Initial values from prior\n        rng_key = jax.random.PRNGKey(0)\n        init_params = self.wrapped_prior.rvs(n_chains)\n        init_dict = {\n            self.wrapped_prior.keys[i]: jnp.array(init_params[:, i]) \n            for i in range(len(self.wrapped_prior.keys))\n        }\n\n        # Use SA (Simulated Annealing) or another gradient-free sampler\n        # because the models are not JAX-traceable.\n        kernel = numpyro.infer.SA(potential_fn=potential_fn)\n        mcmc = MCMC(kernel, num_warmup=n_warmup, num_samples=n_samples, num_chains=n_chains)\n        mcmc.run(rng_key, init_params=init_dict)\n\n        self._save_results(mcmc)\n\n    def _save_results(self, mcmc):\n        samples = mcmc.get_samples()\n        # Convert to pandas. Handle potential chain/sample dimension overlap\n        data = {}\n        for k, v in samples.items():\n            v_np = np.array(v)\n            if v_np.ndim &gt; 1:\n                # Flatten if it's (num_samples, 1) or similar, else keep but pandas might complain\n                if v_np.shape[1] == 1:\n                    data[k] = v_np.flatten()\n                else:\n                    # Multi-dim param, might need special handling but for now just take it\n                    data[k] = list(v_np)\n            else:\n                data[k] = v_np\n\n        df = pd.DataFrame(data)\n\n        df.to_json(self.outdir / f\"{self.label}_result.json\")\n\n        with open(self.outdir / f\"{self.label}_numpyro.pickle\", \"wb\") as f:\n            dill.dump(samples, f)\n</code></pre>"},{"location":"api/#pythiabns.inference.samplers.blackjax_sampler.BlackJAXWrapper","title":"<code>BlackJAXWrapper</code>","text":"<p>Wrapper for BlackJAX sampler.</p> Source code in <code>src/pythiabns/inference/samplers/blackjax_sampler.py</code> <pre><code>class BlackJAXWrapper:\n    \"\"\"Wrapper for BlackJAX sampler.\"\"\"\n\n    def __init__(self, likelihood: bilby.Likelihood, \n                 priors: bilby.core.prior.PriorDict,\n                 outdir: Path,\n                 label: str,\n                 settings: Dict[str, Any] = None):\n\n        self.likelihood = likelihood\n        self.priors = priors\n        self.outdir = Path(outdir)\n        self.label = label\n        self.settings = settings or {}\n\n        self.outdir.mkdir(parents=True, exist_ok=True)\n        self.wrapped_prior = BilbyPocomcPrior(self.priors)\n\n        self.likelihood.parameters.update(self.wrapped_prior.fixed_params)\n\n    def log_likelihood(self, x):\n        params = dict(zip(self.wrapped_prior.keys, x))\n        self.likelihood.parameters.update(params)\n        return self.likelihood.log_likelihood()\n\n    def run(self):\n        import jax\n        import jax.numpy as jnp\n        import blackjax\n\n        n_samples = self.settings.get(\"n_samples\", 1000)\n        n_warmup = self.settings.get(\"n_warmup\", n_samples // 2)\n\n        # BlackJAX is very flexible. Requires a log-density function.\n        def log_density(x):\n             # Wrapper to use pure_callback for non-jax likelihood/prior\n             def wrapped_logp(x_np):\n                 lp = self.wrapped_prior.logpdf(np.atleast_2d(x_np))[0]\n                 ll = self.log_likelihood(x_np)\n                 return lp + ll\n\n             return jax.pure_callback(wrapped_logp, jnp.float64(0.0), x)\n\n        logger.info(f\"BlackJAX sampling initialized with RMH, {n_samples} samples.\")\n\n        # Initial values\n        rng_key = jax.random.PRNGKey(0)\n        init_params = self.wrapped_prior.rvs(1)[0]\n        init_params = jnp.array(init_params)\n\n        # Using Random Walk Metropolis Hastings as it doesn't require gradients\n        # Use additive_step_random_walk with a simple normal step\n        def random_step(key, x):\n            return x + jax.random.normal(key, x.shape) * 0.1\n\n        rw = blackjax.additive_step_random_walk(log_density, random_step)\n        state = rw.init(init_params)\n\n        def step(state, key):\n            state, _ = rw.step(key, state)\n            return state, state\n\n        keys = jax.random.split(rng_key, n_samples)\n        _, states = jax.lax.scan(step, state, keys)\n\n        self._save_results(states.position)\n\n    def _save_results(self, samples):\n        # Convert to pandas\n        df = pd.DataFrame(np.array(samples), columns=self.wrapped_prior.keys)\n        df.to_json(self.outdir / f\"{self.label}_result.json\")\n\n        with open(self.outdir / f\"{self.label}_blackjax.pickle\", \"wb\") as f:\n            dill.dump(samples, f)\n</code></pre>"},{"location":"api/#pythiabns.inference.samplers.tempest.TempestWrapper","title":"<code>TempestWrapper</code>","text":"<p>Wrapper for tempest sampler.</p> Source code in <code>src/pythiabns/inference/samplers/tempest.py</code> <pre><code>class TempestWrapper:\n    \"\"\"Wrapper for tempest sampler.\"\"\"\n\n    def __init__(self, likelihood: bilby.Likelihood, \n                 priors: bilby.core.prior.PriorDict,\n                 outdir: Path,\n                 label: str,\n                 settings: Dict[str, Any] = None):\n\n        self.likelihood = likelihood\n        self.priors = priors\n        self.outdir = Path(outdir)\n        self.label = label\n        self.settings = settings or {}\n\n        self.outdir.mkdir(parents=True, exist_ok=True)\n        self.wrapped_prior = BilbyPocomcPrior(self.priors)\n\n        self.likelihood.parameters.update(self.wrapped_prior.fixed_params)\n\n    def log_likelihood(self, x):\n        params = dict(zip(self.wrapped_prior.keys, x))\n        self.likelihood.parameters.update(params)\n        return self.likelihood.log_likelihood()\n\n    def run(self):\n        import tempest as tp\n        import multiprocess\n\n        # Extract settings for run/pool\n        n_samples = self.settings.get(\"n_samples\", 1000)\n        n_cpus = self.settings.get(\"n_cpus\", 1)\n\n        # Filter settings for Sampler.__init__\n        sampler_settings = self.settings.copy()\n        sampler_settings.pop(\"n_samples\", None)\n        sampler_settings.pop(\"n_cpus\", None)\n\n        # Tempest requires a prior_transform (unit cube -&gt; physical)\n        def prior_transform(u):\n            # Bilby priors rescale takes unit cube values\n            return self.priors.rescale(self.wrapped_prior.keys, u)\n\n        logger.info(f\"Tempest sampling started with {n_samples} samples.\")\n\n        with multiprocess.Pool(n_cpus) as pool:\n            sampler = tp.Sampler(\n                prior_transform=prior_transform,\n                log_likelihood=self.log_likelihood,\n                n_dim=self.wrapped_prior.dim,\n                pool=pool,\n                **sampler_settings\n            )\n\n            sampler.run(n_total=n_samples)\n\n            self._save_results(sampler)\n\n    def _save_results(self, sampler):\n        results = sampler.results()\n        samples = results.get('samples')\n\n        if samples is not None:\n            df = pd.DataFrame(samples, columns=self.wrapped_prior.keys)\n            df['log_likelihood'] = results.get('log_likelihood')\n            df['log_prior'] = results.get('log_prior')\n\n            df.to_json(self.outdir / f\"{self.label}_result.json\")\n\n        with open(self.outdir / f\"{self.label}_tempest.pickle\", \"wb\") as f:\n            dill.dump(results, f)\n</code></pre>"},{"location":"api/#detectors","title":"Detectors","text":""},{"location":"api/#pythiabns.detectors.network.DetectorNetwork","title":"<code>DetectorNetwork</code>","text":"<p>Wrapper around bilby InterferometerList.</p> Source code in <code>src/pythiabns/detectors/network.py</code> <pre><code>class DetectorNetwork:\n    \"\"\"Wrapper around bilby InterferometerList.\"\"\"\n\n    def __init__(self, ifo_names: List[str] = [\"H1\", \"L1\", \"V1\"], \n                 sampling_frequency: float = 4096,\n                 duration: float = 1.0,\n                 start_time: float = 0.0):\n\n        self.ifo_names = ifo_names\n        self.sampling_frequency = sampling_frequency\n        self.duration = duration\n        self.start_time = start_time\n\n        self.ifos = bilby.gw.detector.InterferometerList(ifo_names)\n        self._configure_detectors()\n\n    def _configure_detectors(self):\n        for ifo in self.ifos:\n            ifo.minimum_frequency = 20 # Configurable?\n            ifo.maximum_frequency = self.sampling_frequency / 2.0\n\n    def set_data(self, noise: bool = False):\n        if noise:\n            try:\n                self.ifos.set_strain_data_from_power_spectral_densities(\n                    sampling_frequency=self.sampling_frequency,\n                    duration=self.duration,\n                    start_time=self.start_time\n                )\n            except Exception as e:\n                logger.warning(f\"Failed to set noise from PSD: {e}. Fallback to zero noise + Gaussian?\")\n                # Logic from ifo.py line 150 used set_strain_data_from_power_spectral_densities\n                # This generates Gaussian noise colored by PSD.\n                raise e\n        else:\n            self.ifos.set_strain_data_from_zero_noise(\n                sampling_frequency=self.sampling_frequency,\n                duration=self.duration,\n                start_time=self.start_time\n            )\n\n    def inject_signal(self, waveform_generator: bilby.gw.waveform_generator.WaveformGenerator, parameters: Dict):\n        \"\"\"Inject signal into detectors.\"\"\"\n        self.ifos.inject_signal(\n            waveform_generator=waveform_generator,\n            parameters=parameters,\n            raise_error=False\n        )\n\n    @property\n    def meta_data(self):\n         # Helper to access SNR etc\n         # bilby ifo.meta_data usually stores optimal_SNR after injection\n         return {ifo.name: ifo.meta_data for ifo in self.ifos}\n</code></pre>"},{"location":"api/#pythiabns.detectors.network.DetectorNetwork.inject_signal","title":"<code>inject_signal(waveform_generator, parameters)</code>","text":"<p>Inject signal into detectors.</p> Source code in <code>src/pythiabns/detectors/network.py</code> <pre><code>def inject_signal(self, waveform_generator: bilby.gw.waveform_generator.WaveformGenerator, parameters: Dict):\n    \"\"\"Inject signal into detectors.\"\"\"\n    self.ifos.inject_signal(\n        waveform_generator=waveform_generator,\n        parameters=parameters,\n        raise_error=False\n    )\n</code></pre>"},{"location":"api/#data-utils","title":"Data Utils","text":""},{"location":"api/#pythiabns.data_utils.nr.NumericalWaveform","title":"<code>NumericalWaveform</code>","text":"<p>Class to handle loading and processing of NR waveforms.</p> Source code in <code>src/pythiabns/data_utils/nr.py</code> <pre><code>class NumericalWaveform:\n    \"\"\"Class to handle loading and processing of NR waveforms.\"\"\"\n\n    def __init__(self, filename: str, sampling_frequency: Optional[float] = None):\n        self.filename = filename\n        self.sampling_frequency = sampling_frequency\n\n        # Determine path\n        # Logic ported from NR_strains.py: \n        # If 'Soultanis' in name, special handling. Else scan STRAIN_PATH.\n\n        if os.path.isabs(filename) or os.path.exists(filename):\n            self._load_from_path(Path(filename))\n        elif filename.startswith('Soultanis'):\n            self._load_soultanis(filename)\n        else:\n            self._load_standard(filename)\n\n        # Common initialization\n        self.Mtot = (self.m1 + self.m2) * constants.MSUN_SI\n\n        # Convert to SI\n        self._time_to_SI()\n        self.hp, self.hc = self._set_to_1Mpc()\n\n        # Resample if needed\n        # In original, resampling was mandatory?\n        # self.resample() # TODO: Verify if mandatory\n\n    def _load_from_path(self, path: Path):\n        self.datapath = path\n        # Check if it's an NR directory\n        if (path / \"metadata.txt\").exists():\n            self.metadata_dict = self._load_metadata(self.datapath)\n            self.m1 = float(self.metadata_dict.get(\"id_mass_starA\", 1.4))\n            self.m2 = float(self.metadata_dict.get(\"id_mass_starB\", 1.4))\n            self.rh_overmtot_p, self.rh_overmtot_c, self.time, self.extraction_radius = \\\n                self._read_hdf5_data()\n        else:\n            # Simple file loading (txt/csv)\n            self._load_simple_file(path)\n\n    def _load_standard(self, filename: str):\n        self.datapath = constants.STRAIN_PATH / filename\n        self._load_from_path(self.datapath)\n\n    def _load_simple_file(self, path: Path):\n        # Assume 3 columns: time, hp, hc\n        data = np.loadtxt(path)\n        self.time = data[:, 0]\n        self.rh_overmtot_p = data[:, 1]\n        self.rh_overmtot_c = data[:, 2]\n        self.metadata_dict = {\"id_name\": path.name}\n        self.m1 = 1.4 # Defaults\n        self.m2 = 1.4\n        self.extraction_radius = 0\n\n        # Flag to indicate its already SI and scaled to 1Mpc?\n        # For simplicity, if loading a custom file, we assume it's h+ and hx at 1Mpc and in SI.\n        # So we skip _time_to_SI and _set_to_1Mpc logic by setting special values.\n        self._is_si = True\n\n    def _load_soultanis(self, filename: str):\n        # Soultanis/1.55\n        mass = float(filename.split('/')[-1])\n        base_dir = constants.STRAIN_PATH / filename.split('/')[0]\n\n        # Find matching file?\n        # Original: [i for i in os.listdir(...) if mass in i][0]\n        # Assuming filename structure matches\n        try:\n             # This is a bit fragile but ports existing logic\n             candidates = list(base_dir.glob(f\"*{mass}*\"))\n             if not candidates:\n                 raise FileNotFoundError(f\"No file found for {filename}\")\n             self.datapath = candidates[0]\n        except Exception as e:\n            raise FileNotFoundError(f\"Error finding Soultanis file: {e}\")\n\n        self.metadata_dict = {\n            'id_mass_starA': mass, \n            'id_mass_starB': mass, \n            'id_eos': 'MPA1',\n            'id_name': filename\n        }\n        self.m1 = mass\n        self.m2 = mass\n        self.extraction_radius = 0 # Not applicable/available?\n\n        data = np.loadtxt(self.datapath)\n        self.time = data.T[0] / 1000.0 # ms to s\n        # Original scaling: data.T[1]/8.35898e+20*40\n        # What is 8.358...? Likely unit conversion.\n        # Original comment: #@ 1Mpc\n        # Note: In _set_to_1Mpc we might re-scale. \n        # But Soultanis load seems to return hp already scaled?\n        # Original NumericalData inits rh_overmtot for standard, but hp for Soultanis.\n\n        # Here I will populate rh_overmtot_p/c assuming they are NOT scaled to Mtot yet?\n        # Actually Soultanis loader in original SETS hp directly.\n        # So I should handle that difference.\n\n        self.rh_overmtot_p = data.T[1] / 8.35898e+20 * 40\n        self.rh_overmtot_c = data.T[2] / 8.35898e+20 * 40\n\n        # Hack: set a flag to skip SI conversion if already in SI?\n        # Original: time_msun... \n        # Standard: load_NR_strains -&gt; rh... then time_to_SI -&gt; set_to_1Mpc\n        # Soultanis: loads already converted time?\n        # Original: time = data.T[0]/1000 (ms to s). So it IS in SI (seconds).\n        # Standard loads geometric time?\n\n    def _read_hdf5_data(self):\n        h5_path = self.datapath / \"data.h5\"\n        with h5py.File(h5_path, \"r\") as f:\n             # List l=2, m=2 modes\n             names = [x for x in f[\"/rh_22\"] if \"l2_m2\" in x]\n             # Select extraction at largest radius\n             # Original logic: last one, check for Inf\n             names.sort() # Ensure order?\n             # Original used list(f[]) which is unordered in some h5py versions?\n             # Assuming sorted by string works for radii r100, r200 etc.\n             selection = names[-1]\n             if \"Inf\" in selection and len(names) &gt; 1:\n                 selection = names[-2]\n\n             dset = f[f\"/rh_22/{selection}\"]\n             data = pd.DataFrame(dset[:]) # Read all\n\n             time = data.iloc[:, 0].values\n             rh_p = data.iloc[:, 1].values\n             rh_c = data.iloc[:, 2].values\n\n             # Extract radius from name \"l2_m2_r400.txt\" or similar\n             # Original: float(selection.split(\".\")[0].split(\"r\")[1])\n             try:\n                extraction_radius = float(selection.split(\"r\")[-1].split(\".\")[0])\n             except:\n                extraction_radius = 0.0\n\n             return rh_p, rh_c, time, extraction_radius\n\n    def _load_metadata(self, path: Path) -&gt; Dict[str, Any]:\n        meta_file = path / \"metadata.txt\"\n        meta = {}\n        if not meta_file.exists():\n            return meta\n\n        with open(meta_file, \"r\") as f:\n            for line in f:\n                parts = line.split()\n                if not parts: continue\n                if 'Evolution' in parts: break # Stop reading\n                if \"id_\" in parts[0]:\n                    key = parts[0]\n                    val = parts[-1]\n                    try:\n                        val = float(val)\n                    except:\n                        pass\n                    meta[key] = val\n        return meta\n\n    def _time_to_SI(self):\n        if hasattr(self, \"_is_si\") and self._is_si: return\n        if self.filename.startswith(\"Soultanis\"): return # Already SI\n        # Convert geometric time to seconds\n        # time_SI = time_geom * G * M / c^3\n        factor = constants.G_SI * self.Mtot / (constants.C_SI**3)\n        self.time = self.time * factor\n\n    def _set_to_1Mpc(self):\n        if hasattr(self, \"_is_si\") and self._is_si: \n            return self.rh_overmtot_p, self.rh_overmtot_c\n        if self.filename.startswith(\"Soultanis\"):\n             # Already scaled? Original code set self.hp directly.\n             # In my class I stored it in rh_overmtot for consistency of storage.\n             # So just returns them.\n             return self.rh_overmtot_p, self.rh_overmtot_c\n\n        # Standard scaling\n        # hp = (rh/M) * (G*M/c^2) * (1/dist)\n        # rh_overmtot is actually r*h / Mtot ???\n        # Original: rh_overmtot_p * mtot_geom / 1Mpc\n        # mtot_geom = G * M / c^2 (Length)\n\n        mtot_geom = constants.G_SI * self.Mtot / (constants.C_SI**2)\n        one_mpc = 1e6 * 3.085677581e16 # Parsec to meters\n\n        hp = self.rh_overmtot_p * mtot_geom / one_mpc\n        hc = self.rh_overmtot_c * mtot_geom / one_mpc\n        return hp, hc\n\n    def get_post_merger(self, inplace=True):\n        \"\"\"Crop to post-merger signal.\"\"\"\n        # Find merger time (max amplitude)\n        amp = np.sqrt(self.hp**2 + self.hc**2)\n        idx = np.argmax(amp)\n\n        if inplace:\n            self.time = self.time[idx:]\n            self.hp = self.hp[idx:]\n            self.hc = self.hc[idx:]\n            # Ensure odd/even length consistency? Original had some check\n        else:\n            return self.time[idx:], self.hp[idx:], self.hc[idx:]\n\n    def resample(self, new_fs=None):\n        if new_fs is None:\n             if self.sampling_frequency: new_fs = self.sampling_frequency\n             else: new_fs = 8192\n\n        dt = 1.0/new_fs\n        new_time = np.arange(self.time[0], self.time[-1], dt)\n        self.hp = processing.interpolate(self.time, self.hp, new_time)\n        self.hc = processing.interpolate(self.time, self.hc, new_time)\n        self.time = new_time\n</code></pre>"},{"location":"api/#pythiabns.data_utils.nr.NumericalWaveform.get_post_merger","title":"<code>get_post_merger(inplace=True)</code>","text":"<p>Crop to post-merger signal.</p> Source code in <code>src/pythiabns/data_utils/nr.py</code> <pre><code>def get_post_merger(self, inplace=True):\n    \"\"\"Crop to post-merger signal.\"\"\"\n    # Find merger time (max amplitude)\n    amp = np.sqrt(self.hp**2 + self.hc**2)\n    idx = np.argmax(amp)\n\n    if inplace:\n        self.time = self.time[idx:]\n        self.hp = self.hp[idx:]\n        self.hc = self.hc[idx:]\n        # Ensure odd/even length consistency? Original had some check\n    else:\n        return self.time[idx:], self.hp[idx:], self.hc[idx:]\n</code></pre>"},{"location":"features/","title":"Features","text":""},{"location":"features/#output-structure-smart-naming","title":"Output Structure &amp; Smart Naming","text":"<p>PythiaBNS automatically organizes results to keep large campaigns structured.</p> <ul> <li>Study Folder: Named after the configuration file (or the <code>name</code> field).</li> <li>Run Subfolders: Each simulation gets a dedicated subfolder with an informative name, automatically generated from the varying parameters (e.g., <code>run001_inj_three_sines_snr50.0_model_three_sines</code>).</li> </ul> <p>Resulting Directory Tree:</p> <pre><code>results/\n\u2514\u2500\u2500 MyStudy/\n    \u251c\u2500\u2500 run000_inj_three_sines_snr50.0_model_A/\n    \u251c\u2500\u2500 run001_inj_three_sines_snr100.0_model_A/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"features/#automated-plotting","title":"Automated Plotting","text":"<p>You can configure automated plot generation directly in your YAML config. Plots are generated at the end of each simulation and saved in the respective run folder.</p>"},{"location":"features/#configuration-example","title":"Configuration Example","text":"<pre><code>plotting:\n  enabled: true\n  plots: [\"corner\", \"trace\"] # Supported: corner, trace\n  settings:\n    corner:\n      show_titles: true\n      quantiles: [0.16, 0.5, 0.84]\n    trace:\n      dpi: 150\n</code></pre>"},{"location":"features/#sampler-plugins","title":"Sampler Plugins","text":"<p>PythiaBNS supports multiple sampling backends via its plugin system.</p> Sampler Plugin Name Type Status PocoMC <code>pocomc</code> Preconditioned MC \u2705 Functional Zeus <code>zeus</code> Ensemble Slice \u2705 Functional NumPyro <code>numpyro</code> JAX (SA) \u2705 Functional BlackJAX <code>blackjax</code> JAX (RWM) \u2705 Functional Tempest <code>tempest</code> Persistent Sampler \u2705 Functional Stan <code>stan</code> HMC/NUTS \ud83c\udfd7\ufe0f Wrapper (Plugin) Nutpie <code>nutpie</code> Rust NUTS \ud83c\udfd7\ufe0f Wrapper (Plugin) Bilby Natives <code>dynesty</code>, etc. Various \u2705 Functional"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#running-the-pipeline","title":"Running the Pipeline","text":"<p>The main orchestrator is <code>spine.py</code>. It reads the config, expands the job matrix, and executes simulations.</p> <pre><code># Run with config using uv\nuv run python src/pythiabns/spine.py config.yaml\n</code></pre>"},{"location":"usage/#configuration","title":"Configuration","text":"<p>Simulations are configured using YAML files, validated by Pydantic schemas.</p>"},{"location":"usage/#example-configyaml","title":"Example <code>config.yaml</code>","text":"<pre><code>name: \"Experiment_01\"\noutput_dir: \"results/run1\"\n\n# Import custom plugins (optional)\nimports: \n  - \"my_custom_models\" \n\nmatrix:\n  # Modular Injection\n  injection:\n    - mode: \"nr\"\n      target: \"BAM:0088:R01\" \n    - mode: \"file\" \n      target: \"/path/to/my/waveform/folder\"\n    - mode: \"analytic\"\n      target: \"three_sines\"\n\n  snr: [50.0, 100.0]\n  model: [\"easter_half_reparam\"] \n\n  sampler:\n    plugin: \"pocomc\"\n    settings:\n      npoints: 1000\n      corr_threshold: 0.75\n      n_cpus: 16\n\n  priors:\n    mode: \"file\" # or \"empirical\" to use relations\n    source: \"easter_half_reparam.priors\"\n\n  # Specific model arguments (passed to get_model)\n  model_params:\n      nfreqs: 3\n</code></pre>"},{"location":"usage/#combinatorial-matrix","title":"Combinatorial Matrix","text":"<p>PythiaBNS is designed to make parameter studies effortless through its combinatorial matrix system. Any argument in the <code>matrix</code> section of the configuration file can be provided as a list. The pipeline will automatically generate and run simulations for all Cartesian products of these lists.</p>"},{"location":"usage/#example-study","title":"Example Study","text":"<pre><code>matrix:\n  # ...\n  snr: [20, 50, 100]            # 3 values\n  model: [\"model_A\", \"model_B\"] # 2 models\n  sampler:\n    plugin: \"pocomc\"\n    settings:\n      n_cpus: [8, 16]           # 2 settings\n</code></pre> <p>This configuration will automatically trigger 3 \u00d7 2 \u00d7 2 = 12 distinct simulations, covering every combination of SNR, model, and CPU count.</p>"}]}